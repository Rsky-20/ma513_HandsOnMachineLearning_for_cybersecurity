{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p align=\"center\"> <img src=\"https://upload.wikimedia.org/wikipedia/en/thumb/f/f1/Logo-IPSA.png/200px-Logo-IPSA.png\" alt=\"IPSA Logo\" style=\"width:150px;height:auto;\"> </p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# IPSA - Ma 513 - Hands-on Machine Learning for Cybersecurity\n",
    "\n",
    "## Named Entity Recognition for Cybersecurity\n",
    "\n",
    "### Projet infos\n",
    "Teacher : \n",
    "Student : Valentin DESFORGES (Valouuuu24) ; Sylvain LAGARENNE () ; Pierre VAUDRY (Rsky-20) \n",
    "\n",
    "### Project Overview\n",
    "As part of the Statistical Learning course at IPSA, this project focuses on Named Entity Recognition (NER) in the cybersecurity domain. The main objective is to develop a system capable of recognizing and classifying critical cybersecurity-related entities in text, such as malware, attacks, and threat actors, using state-of-the-art Natural Language Processing (NLP) techniques.\n",
    "\n",
    "### Dataset\n",
    "The dataset is sourced from SemEval-2018 Task 8 (\"SecureNLP\") and is formatted in JSON Lines. Each entry contains:\n",
    "\n",
    "- unique_id: Unique identifier for the sentence.\n",
    "- tokens: List of tokens (strings) forming the text.\n",
    "- ner_tags: Named Entity Recognition tags following the IOB2 convention.\n",
    "\n",
    "Example Entry:\n",
    "\n",
    "[json format]\n",
    "> {\n",
    ">\n",
    ">   \"unique_id\": 4775,\n",
    ">\n",
    ">   \"tokens\": [\"This\", \"collects\", \":\", \"Collected\", \"data\", \"will\", \"be\", \"uploaded\", \"to\", \"a\", \"DynDNS\", \"domain\", \"currently\", \"hosted\", \"on\", \"a\", \"US\", \"webhosting\", \"service\", \".\"],\n",
    ">\n",
    ">  \"ner_tags\": [\"B-Entity\", \"B-Action\", \"O\", \"B-Entity\", \"I-Entity\", \"O\", \"B-Action\", \"I-Action\", \"B-Modifier\", \"B-Entity\", \"I-Entity\", \"I-Entity\", \"I-Entity\", \"I-Entity\", \"I-Entity\", \"I-Entity\", \"I-Entity\", \"I-Entity\", \"I-Entity\", \"O\"]\n",
    "> \n",
    ">}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "v1Rbw7cIu5us"
   },
   "source": [
    "Named entity recognition (NER): Find the entities (such as persons, locations, or organizations) in a sentence. This can be formulated as attributing a label to each token by having one class per entity and one class for “no entity.”\n",
    "\n",
    "Part-of-speech tagging (POS): Mark each word in a sentence as corresponding to a particular part of speech (such as noun, verb, adjective, etc.).\n",
    "\n",
    "Chunking: Find the tokens that belong to the same entity. This task (which can be combined with POS or NER) can be formulated as attributing one label (usually B-) to any tokens that are at the beginning of a chunk, another label (usually I-) to tokens that are inside a chunk, and a third label (usually O) to tokens that don’t belong to any chunk.\n",
    "\n",
    "O means the word doesn’t correspond to any entity.\n",
    "\n",
    "B-PER/I-PER means the word corresponds to the beginning of/is inside a person entity.\n",
    "\n",
    "B-ORG/I-ORG means the word corresponds to the beginning of/is inside an organization entity.\n",
    "\n",
    "B-LOC/I-LOC means the word corresponds to the beginning of/is inside a location entity.\n",
    "\n",
    "B-MISC/I-MISC means the word corresponds to the beginning of/is inside a miscellaneous entity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "1LlK0DO6mh8u"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://download.pytorch.org/whl/cu124\n",
      "Requirement already satisfied: torch in e:\\programmation\\python\\.ma511\\lib\\site-packages (2.5.1+cu121)\n",
      "Requirement already satisfied: torchvision in e:\\programmation\\python\\.ma511\\lib\\site-packages (0.20.1+cu121)\n",
      "Requirement already satisfied: torchaudio in e:\\programmation\\python\\.ma511\\lib\\site-packages (2.5.1+cu121)\n",
      "Requirement already satisfied: filelock in e:\\programmation\\python\\.ma511\\lib\\site-packages (from torch) (3.16.1)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in e:\\programmation\\python\\.ma511\\lib\\site-packages (from torch) (4.12.2)\n",
      "Requirement already satisfied: networkx in e:\\programmation\\python\\.ma511\\lib\\site-packages (from torch) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in e:\\programmation\\python\\.ma511\\lib\\site-packages (from torch) (3.1.4)\n",
      "Requirement already satisfied: fsspec in e:\\programmation\\python\\.ma511\\lib\\site-packages (from torch) (2024.9.0)\n",
      "Requirement already satisfied: setuptools in e:\\programmation\\python\\.ma511\\lib\\site-packages (from torch) (75.6.0)\n",
      "Requirement already satisfied: sympy==1.13.1 in e:\\programmation\\python\\.ma511\\lib\\site-packages (from torch) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in e:\\programmation\\python\\.ma511\\lib\\site-packages (from sympy==1.13.1->torch) (1.3.0)\n",
      "Requirement already satisfied: numpy in e:\\programmation\\python\\.ma511\\lib\\site-packages (from torchvision) (2.0.2)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in e:\\programmation\\python\\.ma511\\lib\\site-packages (from torchvision) (11.0.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in e:\\programmation\\python\\.ma511\\lib\\site-packages (from jinja2->torch) (3.0.2)\n",
      "Requirement already satisfied: accelerate in e:\\programmation\\python\\.ma511\\lib\\site-packages (from -r requirements.txt (line 1)) (1.2.1)\n",
      "Requirement already satisfied: aiohappyeyeballs in e:\\programmation\\python\\.ma511\\lib\\site-packages (from -r requirements.txt (line 2)) (2.4.4)\n",
      "Requirement already satisfied: aiohttp in e:\\programmation\\python\\.ma511\\lib\\site-packages (from -r requirements.txt (line 3)) (3.11.11)\n",
      "Requirement already satisfied: aiosignal in e:\\programmation\\python\\.ma511\\lib\\site-packages (from -r requirements.txt (line 4)) (1.3.2)\n",
      "Requirement already satisfied: asttokens in e:\\programmation\\python\\.ma511\\lib\\site-packages (from -r requirements.txt (line 5)) (3.0.0)\n",
      "Requirement already satisfied: attrs in e:\\programmation\\python\\.ma511\\lib\\site-packages (from -r requirements.txt (line 6)) (24.3.0)\n",
      "Requirement already satisfied: certifi in e:\\programmation\\python\\.ma511\\lib\\site-packages (from -r requirements.txt (line 7)) (2024.12.14)\n",
      "Requirement already satisfied: charset-normalizer in e:\\programmation\\python\\.ma511\\lib\\site-packages (from -r requirements.txt (line 8)) (3.4.0)\n",
      "Requirement already satisfied: click in e:\\programmation\\python\\.ma511\\lib\\site-packages (from -r requirements.txt (line 9)) (8.1.7)\n",
      "Requirement already satisfied: colorama in e:\\programmation\\python\\.ma511\\lib\\site-packages (from -r requirements.txt (line 10)) (0.4.6)\n",
      "Requirement already satisfied: comm in e:\\programmation\\python\\.ma511\\lib\\site-packages (from -r requirements.txt (line 11)) (0.2.2)\n",
      "Requirement already satisfied: contourpy in e:\\programmation\\python\\.ma511\\lib\\site-packages (from -r requirements.txt (line 12)) (1.3.1)\n",
      "Requirement already satisfied: cycler in e:\\programmation\\python\\.ma511\\lib\\site-packages (from -r requirements.txt (line 13)) (0.12.1)\n",
      "Requirement already satisfied: datasets in e:\\programmation\\python\\.ma511\\lib\\site-packages (from -r requirements.txt (line 14)) (3.2.0)\n",
      "Requirement already satisfied: debugpy in e:\\programmation\\python\\.ma511\\lib\\site-packages (from -r requirements.txt (line 15)) (1.8.11)\n",
      "Requirement already satisfied: decorator in e:\\programmation\\python\\.ma511\\lib\\site-packages (from -r requirements.txt (line 16)) (5.1.1)\n",
      "Requirement already satisfied: dill in e:\\programmation\\python\\.ma511\\lib\\site-packages (from -r requirements.txt (line 17)) (0.3.8)\n",
      "Requirement already satisfied: evaluate in e:\\programmation\\python\\.ma511\\lib\\site-packages (from -r requirements.txt (line 18)) (0.4.3)\n",
      "Requirement already satisfied: executing in e:\\programmation\\python\\.ma511\\lib\\site-packages (from -r requirements.txt (line 19)) (2.1.0)\n",
      "Requirement already satisfied: filelock in e:\\programmation\\python\\.ma511\\lib\\site-packages (from -r requirements.txt (line 20)) (3.16.1)\n",
      "Requirement already satisfied: fonttools in e:\\programmation\\python\\.ma511\\lib\\site-packages (from -r requirements.txt (line 21)) (4.55.3)\n",
      "Requirement already satisfied: frozenlist in e:\\programmation\\python\\.ma511\\lib\\site-packages (from -r requirements.txt (line 22)) (1.5.0)\n",
      "Requirement already satisfied: fsspec in e:\\programmation\\python\\.ma511\\lib\\site-packages (from -r requirements.txt (line 23)) (2024.9.0)\n",
      "Requirement already satisfied: graphviz in e:\\programmation\\python\\.ma511\\lib\\site-packages (from -r requirements.txt (line 24)) (0.20.3)\n",
      "Requirement already satisfied: huggingface-hub in e:\\programmation\\python\\.ma511\\lib\\site-packages (from -r requirements.txt (line 25)) (0.27.0)\n",
      "Requirement already satisfied: idna in e:\\programmation\\python\\.ma511\\lib\\site-packages (from -r requirements.txt (line 26)) (3.10)\n",
      "Requirement already satisfied: ipykernel in e:\\programmation\\python\\.ma511\\lib\\site-packages (from -r requirements.txt (line 27)) (6.29.5)\n",
      "Requirement already satisfied: ipython in e:\\programmation\\python\\.ma511\\lib\\site-packages (from -r requirements.txt (line 28)) (8.30.0)\n",
      "Requirement already satisfied: jedi in e:\\programmation\\python\\.ma511\\lib\\site-packages (from -r requirements.txt (line 29)) (0.19.2)\n",
      "Requirement already satisfied: Jinja2 in e:\\programmation\\python\\.ma511\\lib\\site-packages (from -r requirements.txt (line 30)) (3.1.4)\n",
      "Requirement already satisfied: joblib in e:\\programmation\\python\\.ma511\\lib\\site-packages (from -r requirements.txt (line 31)) (1.4.2)\n",
      "Requirement already satisfied: jupyter_client in e:\\programmation\\python\\.ma511\\lib\\site-packages (from -r requirements.txt (line 32)) (8.6.3)\n",
      "Requirement already satisfied: jupyter_core in e:\\programmation\\python\\.ma511\\lib\\site-packages (from -r requirements.txt (line 33)) (5.7.2)\n",
      "Requirement already satisfied: kiwisolver in e:\\programmation\\python\\.ma511\\lib\\site-packages (from -r requirements.txt (line 34)) (1.4.7)\n",
      "Requirement already satisfied: markovify in e:\\programmation\\python\\.ma511\\lib\\site-packages (from -r requirements.txt (line 35)) (0.9.4)\n",
      "Requirement already satisfied: MarkupSafe in e:\\programmation\\python\\.ma511\\lib\\site-packages (from -r requirements.txt (line 36)) (3.0.2)\n",
      "Requirement already satisfied: matplotlib in e:\\programmation\\python\\.ma511\\lib\\site-packages (from -r requirements.txt (line 37)) (3.10.0)\n",
      "Requirement already satisfied: matplotlib-inline in e:\\programmation\\python\\.ma511\\lib\\site-packages (from -r requirements.txt (line 38)) (0.1.7)\n",
      "Requirement already satisfied: mlxtend in e:\\programmation\\python\\.ma511\\lib\\site-packages (from -r requirements.txt (line 39)) (0.23.3)\n",
      "Requirement already satisfied: mpmath in e:\\programmation\\python\\.ma511\\lib\\site-packages (from -r requirements.txt (line 40)) (1.3.0)\n",
      "Requirement already satisfied: multidict in e:\\programmation\\python\\.ma511\\lib\\site-packages (from -r requirements.txt (line 41)) (6.1.0)\n",
      "Requirement already satisfied: multiprocess in e:\\programmation\\python\\.ma511\\lib\\site-packages (from -r requirements.txt (line 42)) (0.70.16)\n",
      "Requirement already satisfied: nest-asyncio in e:\\programmation\\python\\.ma511\\lib\\site-packages (from -r requirements.txt (line 43)) (1.6.0)\n",
      "Requirement already satisfied: networkx in e:\\programmation\\python\\.ma511\\lib\\site-packages (from -r requirements.txt (line 44)) (3.4.2)\n",
      "Requirement already satisfied: nltk in e:\\programmation\\python\\.ma511\\lib\\site-packages (from -r requirements.txt (line 45)) (3.9.1)\n",
      "Requirement already satisfied: numpy in e:\\programmation\\python\\.ma511\\lib\\site-packages (from -r requirements.txt (line 46)) (2.0.2)\n",
      "Requirement already satisfied: packaging in e:\\programmation\\python\\.ma511\\lib\\site-packages (from -r requirements.txt (line 47)) (24.2)\n",
      "Requirement already satisfied: pandas in e:\\programmation\\python\\.ma511\\lib\\site-packages (from -r requirements.txt (line 48)) (2.2.3)\n",
      "Requirement already satisfied: parso in e:\\programmation\\python\\.ma511\\lib\\site-packages (from -r requirements.txt (line 49)) (0.8.4)\n",
      "Requirement already satisfied: patsy in e:\\programmation\\python\\.ma511\\lib\\site-packages (from -r requirements.txt (line 50)) (1.0.1)\n",
      "Requirement already satisfied: pillow in e:\\programmation\\python\\.ma511\\lib\\site-packages (from -r requirements.txt (line 51)) (11.0.0)\n",
      "Requirement already satisfied: platformdirs in e:\\programmation\\python\\.ma511\\lib\\site-packages (from -r requirements.txt (line 52)) (4.3.6)\n",
      "Requirement already satisfied: prompt_toolkit in e:\\programmation\\python\\.ma511\\lib\\site-packages (from -r requirements.txt (line 53)) (3.0.48)\n",
      "Requirement already satisfied: propcache in e:\\programmation\\python\\.ma511\\lib\\site-packages (from -r requirements.txt (line 54)) (0.2.1)\n",
      "Requirement already satisfied: psutil in e:\\programmation\\python\\.ma511\\lib\\site-packages (from -r requirements.txt (line 55)) (6.1.0)\n",
      "Requirement already satisfied: pure_eval in e:\\programmation\\python\\.ma511\\lib\\site-packages (from -r requirements.txt (line 56)) (0.2.3)\n",
      "Requirement already satisfied: pyarrow in e:\\programmation\\python\\.ma511\\lib\\site-packages (from -r requirements.txt (line 57)) (18.1.0)\n",
      "Requirement already satisfied: pydotplus in e:\\programmation\\python\\.ma511\\lib\\site-packages (from -r requirements.txt (line 58)) (2.0.2)\n",
      "Requirement already satisfied: Pygments in e:\\programmation\\python\\.ma511\\lib\\site-packages (from -r requirements.txt (line 59)) (2.18.0)\n",
      "Requirement already satisfied: pyparsing in e:\\programmation\\python\\.ma511\\lib\\site-packages (from -r requirements.txt (line 60)) (3.2.0)\n",
      "Requirement already satisfied: python-dateutil in e:\\programmation\\python\\.ma511\\lib\\site-packages (from -r requirements.txt (line 61)) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz in e:\\programmation\\python\\.ma511\\lib\\site-packages (from -r requirements.txt (line 62)) (2024.2)\n",
      "Requirement already satisfied: pywin32 in e:\\programmation\\python\\.ma511\\lib\\site-packages (from -r requirements.txt (line 63)) (308)\n",
      "Requirement already satisfied: PyYAML in e:\\programmation\\python\\.ma511\\lib\\site-packages (from -r requirements.txt (line 64)) (6.0.2)\n",
      "Requirement already satisfied: pyzmq in e:\\programmation\\python\\.ma511\\lib\\site-packages (from -r requirements.txt (line 65)) (26.2.0)\n",
      "Requirement already satisfied: regex in e:\\programmation\\python\\.ma511\\lib\\site-packages (from -r requirements.txt (line 66)) (2024.11.6)\n",
      "Requirement already satisfied: requests in e:\\programmation\\python\\.ma511\\lib\\site-packages (from -r requirements.txt (line 67)) (2.32.3)\n",
      "Requirement already satisfied: safetensors in e:\\programmation\\python\\.ma511\\lib\\site-packages (from -r requirements.txt (line 68)) (0.5.0)\n",
      "Requirement already satisfied: scikit-learn in e:\\programmation\\python\\.ma511\\lib\\site-packages (from -r requirements.txt (line 69)) (1.6.0)\n",
      "Requirement already satisfied: scipy in e:\\programmation\\python\\.ma511\\lib\\site-packages (from -r requirements.txt (line 70)) (1.14.1)\n",
      "Requirement already satisfied: seaborn in e:\\programmation\\python\\.ma511\\lib\\site-packages (from -r requirements.txt (line 71)) (0.13.2)\n",
      "Requirement already satisfied: seqeval in e:\\programmation\\python\\.ma511\\lib\\site-packages (from -r requirements.txt (line 72)) (1.2.2)\n",
      "Requirement already satisfied: setuptools in e:\\programmation\\python\\.ma511\\lib\\site-packages (from -r requirements.txt (line 73)) (75.6.0)\n",
      "Requirement already satisfied: six in e:\\programmation\\python\\.ma511\\lib\\site-packages (from -r requirements.txt (line 74)) (1.17.0)\n",
      "Requirement already satisfied: stack-data in e:\\programmation\\python\\.ma511\\lib\\site-packages (from -r requirements.txt (line 75)) (0.6.3)\n",
      "Requirement already satisfied: statsmodels in e:\\programmation\\python\\.ma511\\lib\\site-packages (from -r requirements.txt (line 76)) (0.14.4)\n",
      "Requirement already satisfied: sympy in e:\\programmation\\python\\.ma511\\lib\\site-packages (from -r requirements.txt (line 77)) (1.13.1)\n",
      "Requirement already satisfied: textblob in e:\\programmation\\python\\.ma511\\lib\\site-packages (from -r requirements.txt (line 78)) (0.18.0.post0)\n",
      "Requirement already satisfied: tf-keras in e:\\programmation\\python\\.ma511\\lib\\site-packages (from -r requirements.txt (line 79)) (2.18.0)\n",
      "Requirement already satisfied: threadpoolctl in e:\\programmation\\python\\.ma511\\lib\\site-packages (from -r requirements.txt (line 80)) (3.5.0)\n",
      "Requirement already satisfied: tokenizers in e:\\programmation\\python\\.ma511\\lib\\site-packages (from -r requirements.txt (line 81)) (0.21.0)\n",
      "Requirement already satisfied: tornado in e:\\programmation\\python\\.ma511\\lib\\site-packages (from -r requirements.txt (line 82)) (6.4.2)\n",
      "Requirement already satisfied: tqdm in e:\\programmation\\python\\.ma511\\lib\\site-packages (from -r requirements.txt (line 83)) (4.67.1)\n",
      "Requirement already satisfied: traitlets in e:\\programmation\\python\\.ma511\\lib\\site-packages (from -r requirements.txt (line 84)) (5.14.3)\n",
      "Requirement already satisfied: transformers in e:\\programmation\\python\\.ma511\\lib\\site-packages (from -r requirements.txt (line 85)) (4.47.1)\n",
      "Requirement already satisfied: typing_extensions in e:\\programmation\\python\\.ma511\\lib\\site-packages (from -r requirements.txt (line 86)) (4.12.2)\n",
      "Requirement already satisfied: tzdata in e:\\programmation\\python\\.ma511\\lib\\site-packages (from -r requirements.txt (line 87)) (2024.2)\n",
      "Requirement already satisfied: Unidecode in e:\\programmation\\python\\.ma511\\lib\\site-packages (from -r requirements.txt (line 88)) (1.3.8)\n",
      "Requirement already satisfied: urllib3 in e:\\programmation\\python\\.ma511\\lib\\site-packages (from -r requirements.txt (line 89)) (2.2.3)\n",
      "Requirement already satisfied: wcwidth in e:\\programmation\\python\\.ma511\\lib\\site-packages (from -r requirements.txt (line 90)) (0.2.13)\n",
      "Requirement already satisfied: xxhash in e:\\programmation\\python\\.ma511\\lib\\site-packages (from -r requirements.txt (line 91)) (3.5.0)\n",
      "Requirement already satisfied: yarl in e:\\programmation\\python\\.ma511\\lib\\site-packages (from -r requirements.txt (line 92)) (1.18.3)\n",
      "Requirement already satisfied: torch>=1.10.0 in e:\\programmation\\python\\.ma511\\lib\\site-packages (from accelerate->-r requirements.txt (line 1)) (2.5.1+cu121)\n",
      "Requirement already satisfied: tensorflow<2.19,>=2.18 in e:\\programmation\\python\\.ma511\\lib\\site-packages (from tf-keras->-r requirements.txt (line 79)) (2.18.0)\n",
      "Requirement already satisfied: tensorflow-intel==2.18.0 in e:\\programmation\\python\\.ma511\\lib\\site-packages (from tensorflow<2.19,>=2.18->tf-keras->-r requirements.txt (line 79)) (2.18.0)\n",
      "Requirement already satisfied: absl-py>=1.0.0 in e:\\programmation\\python\\.ma511\\lib\\site-packages (from tensorflow-intel==2.18.0->tensorflow<2.19,>=2.18->tf-keras->-r requirements.txt (line 79)) (2.1.0)\n",
      "Requirement already satisfied: astunparse>=1.6.0 in e:\\programmation\\python\\.ma511\\lib\\site-packages (from tensorflow-intel==2.18.0->tensorflow<2.19,>=2.18->tf-keras->-r requirements.txt (line 79)) (1.6.3)\n",
      "Requirement already satisfied: flatbuffers>=24.3.25 in e:\\programmation\\python\\.ma511\\lib\\site-packages (from tensorflow-intel==2.18.0->tensorflow<2.19,>=2.18->tf-keras->-r requirements.txt (line 79)) (24.3.25)\n",
      "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in e:\\programmation\\python\\.ma511\\lib\\site-packages (from tensorflow-intel==2.18.0->tensorflow<2.19,>=2.18->tf-keras->-r requirements.txt (line 79)) (0.6.0)\n",
      "Requirement already satisfied: google-pasta>=0.1.1 in e:\\programmation\\python\\.ma511\\lib\\site-packages (from tensorflow-intel==2.18.0->tensorflow<2.19,>=2.18->tf-keras->-r requirements.txt (line 79)) (0.2.0)\n",
      "Requirement already satisfied: libclang>=13.0.0 in e:\\programmation\\python\\.ma511\\lib\\site-packages (from tensorflow-intel==2.18.0->tensorflow<2.19,>=2.18->tf-keras->-r requirements.txt (line 79)) (18.1.1)\n",
      "Requirement already satisfied: opt-einsum>=2.3.2 in e:\\programmation\\python\\.ma511\\lib\\site-packages (from tensorflow-intel==2.18.0->tensorflow<2.19,>=2.18->tf-keras->-r requirements.txt (line 79)) (3.4.0)\n",
      "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.3 in e:\\programmation\\python\\.ma511\\lib\\site-packages (from tensorflow-intel==2.18.0->tensorflow<2.19,>=2.18->tf-keras->-r requirements.txt (line 79)) (5.29.1)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in e:\\programmation\\python\\.ma511\\lib\\site-packages (from tensorflow-intel==2.18.0->tensorflow<2.19,>=2.18->tf-keras->-r requirements.txt (line 79)) (2.5.0)\n",
      "Requirement already satisfied: wrapt>=1.11.0 in e:\\programmation\\python\\.ma511\\lib\\site-packages (from tensorflow-intel==2.18.0->tensorflow<2.19,>=2.18->tf-keras->-r requirements.txt (line 79)) (1.17.0)\n",
      "Requirement already satisfied: grpcio<2.0,>=1.24.3 in e:\\programmation\\python\\.ma511\\lib\\site-packages (from tensorflow-intel==2.18.0->tensorflow<2.19,>=2.18->tf-keras->-r requirements.txt (line 79)) (1.68.1)\n",
      "Requirement already satisfied: tensorboard<2.19,>=2.18 in e:\\programmation\\python\\.ma511\\lib\\site-packages (from tensorflow-intel==2.18.0->tensorflow<2.19,>=2.18->tf-keras->-r requirements.txt (line 79)) (2.18.0)\n",
      "Requirement already satisfied: keras>=3.5.0 in e:\\programmation\\python\\.ma511\\lib\\site-packages (from tensorflow-intel==2.18.0->tensorflow<2.19,>=2.18->tf-keras->-r requirements.txt (line 79)) (3.5.0)\n",
      "Requirement already satisfied: h5py>=3.11.0 in e:\\programmation\\python\\.ma511\\lib\\site-packages (from tensorflow-intel==2.18.0->tensorflow<2.19,>=2.18->tf-keras->-r requirements.txt (line 79)) (3.12.1)\n",
      "Requirement already satisfied: ml-dtypes<0.5.0,>=0.4.0 in e:\\programmation\\python\\.ma511\\lib\\site-packages (from tensorflow-intel==2.18.0->tensorflow<2.19,>=2.18->tf-keras->-r requirements.txt (line 79)) (0.4.1)\n",
      "Requirement already satisfied: wheel<1.0,>=0.23.0 in e:\\programmation\\python\\.ma511\\lib\\site-packages (from astunparse>=1.6.0->tensorflow-intel==2.18.0->tensorflow<2.19,>=2.18->tf-keras->-r requirements.txt (line 79)) (0.45.1)\n",
      "Requirement already satisfied: rich in e:\\programmation\\python\\.ma511\\lib\\site-packages (from keras>=3.5.0->tensorflow-intel==2.18.0->tensorflow<2.19,>=2.18->tf-keras->-r requirements.txt (line 79)) (13.9.4)\n",
      "Requirement already satisfied: namex in e:\\programmation\\python\\.ma511\\lib\\site-packages (from keras>=3.5.0->tensorflow-intel==2.18.0->tensorflow<2.19,>=2.18->tf-keras->-r requirements.txt (line 79)) (0.0.8)\n",
      "Requirement already satisfied: optree in e:\\programmation\\python\\.ma511\\lib\\site-packages (from keras>=3.5.0->tensorflow-intel==2.18.0->tensorflow<2.19,>=2.18->tf-keras->-r requirements.txt (line 79)) (0.13.1)\n",
      "Requirement already satisfied: markdown>=2.6.8 in e:\\programmation\\python\\.ma511\\lib\\site-packages (from tensorboard<2.19,>=2.18->tensorflow-intel==2.18.0->tensorflow<2.19,>=2.18->tf-keras->-r requirements.txt (line 79)) (3.7)\n",
      "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in e:\\programmation\\python\\.ma511\\lib\\site-packages (from tensorboard<2.19,>=2.18->tensorflow-intel==2.18.0->tensorflow<2.19,>=2.18->tf-keras->-r requirements.txt (line 79)) (0.7.2)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in e:\\programmation\\python\\.ma511\\lib\\site-packages (from tensorboard<2.19,>=2.18->tensorflow-intel==2.18.0->tensorflow<2.19,>=2.18->tf-keras->-r requirements.txt (line 79)) (3.1.3)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in e:\\programmation\\python\\.ma511\\lib\\site-packages (from rich->keras>=3.5.0->tensorflow-intel==2.18.0->tensorflow<2.19,>=2.18->tf-keras->-r requirements.txt (line 79)) (3.0.0)\n",
      "Requirement already satisfied: mdurl~=0.1 in e:\\programmation\\python\\.ma511\\lib\\site-packages (from markdown-it-py>=2.2.0->rich->keras>=3.5.0->tensorflow-intel==2.18.0->tensorflow<2.19,>=2.18->tf-keras->-r requirements.txt (line 79)) (0.1.2)\n"
     ]
    }
   ],
   "source": [
    "# install packages\n",
    "! pip install transformers datasets tokenizers seqeval -q\n",
    "! pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu124\n",
    "! pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lsKnxeh6w5Kl"
   },
   "source": [
    "### **Import Libraries**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "0C2hz-NfwyOe"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.5.1+cu121\n",
      "NVIDIA GeForce RTX 4060 Ti\n",
      "WARNING:tensorflow:From e:\\programmation\\python\\.ma511\\Lib\\site-packages\\tf_keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import datasets\n",
    "import json\n",
    "import evaluate\n",
    "import json\n",
    "import numpy as np\n",
    "import transformers\n",
    "import torch\n",
    "print(torch.__version__)  # Vérifie la version de PyTorch\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"{torch.cuda.get_device_name(0)}\")  # Affiche le nom de votre GPU\n",
    "\n",
    "import accelerate\n",
    "import transformers\n",
    "from transformers import pipeline\n",
    "from transformers import TrainingArguments, Trainer\n",
    "from transformers import TrainingArguments\n",
    "from transformers import BertTokenizerFast\n",
    "from transformers import DataCollatorForTokenClassification  # This libary apply augumentation technique at runtime\n",
    "from transformers import AutoModelForTokenClassification     # This class is responsible for load model into my memory\n",
    "from datasets import DatasetDict, Dataset, Features, Sequence, ClassLabel, Value\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Global variable**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "MAX_LEN = 131\n",
    "TRAIN_BATCH_SIZE = 32\n",
    "VALID_BATCH_SIZE = 32\n",
    "EPOCHS = 3\n",
    "LEARNING_RATE = 2e-5\n",
    "BASE_MODEL_PATH = \"dslim/bert-large-NER\"\n",
    "MODEL_PATH = \"./data/bert-base-NER_model\"\n",
    "TRAINING_FILE = \"./data/NER-TRAINING.jsonlines\"\n",
    "VALIDATION_FILE = \"./data/NER-VALIDATION.jsonlines\"\n",
    "TESTING_FILE = \"./data/NER-TESTING.jsonlines\"\n",
    "TESTING_OUTPUT_FILE = \"./data/NER-TESTING-PREDICTED.jsonlines\"\n",
    "VALIDATION_OUTPUT_FILE = \"./data/NER-VALIDATION-PREDICTED.jsonlines\"\n",
    "\n",
    "DEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print(f\"Using device: {DEVICE}\")\n",
    "\n",
    "TOKENIZER = transformers.BertTokenizer.from_pretrained(BASE_MODEL_PATH, do_lower_case=True)\n",
    "# Mapping des labels\n",
    "ID2LABEL = {\n",
    "    0: \"B-Action\",\n",
    "    1: \"B-Entity\",\n",
    "    2: \"B-Modifier\",\n",
    "    3: \"I-Action\",\n",
    "    4: \"I-Entity\",\n",
    "    5: \"I-Modifier\",\n",
    "    6: \"O\"\n",
    "}\n",
    "LABEL2ID = {v: k for k, v in ID2LABEL.items()}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HqK36jJP68IV"
   },
   "source": [
    "### **Load dataset**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "R6RrHJipzTi1"
   },
   "outputs": [],
   "source": [
    "\n",
    "# Charger les fichiers JSONlines et extraire les données\n",
    "def load_and_prepare_data(file_path):\n",
    "    with open(file_path, 'r', encoding='utf-8') as f:\n",
    "        data = [json.loads(line) for line in f]\n",
    "    return data\n",
    "\n",
    "# Convertir une section de données en un objet Dataset avec ClassLabel\n",
    "def convert_to_dataset_with_labels(data_section, labels):\n",
    "    # Vérifier dynamiquement si \"ner_tags\" est présent dans les données\n",
    "    has_ner_tags = \"ner_tags\" in data_section[0]\n",
    "    \n",
    "    # Définir les features dynamiquement\n",
    "    if has_ner_tags:\n",
    "        features = Features({\n",
    "            \"id\": Value(\"int64\"),\n",
    "            \"tokens\": Sequence(Value(\"string\")),\n",
    "            \"ner_tags\": Sequence(ClassLabel(names=labels))\n",
    "        })\n",
    "    else:\n",
    "        features = Features({\n",
    "            \"id\": Value(\"int64\"),\n",
    "            \"tokens\": Sequence(Value(\"string\"))\n",
    "        })\n",
    "    \n",
    "    # Préparer les données\n",
    "    dataset_dict = {\n",
    "        \"id\": [example[\"unique_id\"] for example in data_section],\n",
    "        \"tokens\": [example[\"tokens\"] for example in data_section],\n",
    "    }\n",
    "    if has_ner_tags:\n",
    "        dataset_dict[\"ner_tags\"] = [example[\"ner_tags\"] for example in data_section]\n",
    "\n",
    "    # Créer le dataset\n",
    "    dataset = Dataset.from_dict(dataset_dict, features=features)\n",
    "    return dataset\n",
    "\n",
    "\n",
    "\n",
    "# Charger les données brutes\n",
    "train_data = load_and_prepare_data(TRAINING_FILE)\n",
    "validation_data = load_and_prepare_data(VALIDATION_FILE)\n",
    "test_data = load_and_prepare_data(TESTING_FILE)\n",
    "\n",
    "# Liste des labels\n",
    "ner_labels = [\"B-Action\", \"B-Entity\", \"B-Modifier\", \"I-Action\", \"I-Entity\", \"I-Modifier\", \"O\"]\n",
    "\n",
    "# Créer un DatasetDict avec des labels\n",
    "ner_data = DatasetDict({\n",
    "    \"train\": convert_to_dataset_with_labels(train_data, ner_labels),\n",
    "    \"validation\": convert_to_dataset_with_labels(validation_data, ner_labels),\n",
    "    \"test\": convert_to_dataset_with_labels(test_data, ner_labels)\n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DQvK0kpM-Q22"
   },
   "source": [
    "### **Know Your Data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Fa9gKulI7H5q",
    "outputId": "53afaf4a-6dd7-40f7-832d-3b73db2168d9"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['id', 'tokens', 'ner_tags'],\n",
       "        num_rows: 4876\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['id', 'tokens', 'ner_tags'],\n",
       "        num_rows: 1044\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['id', 'tokens'],\n",
       "        num_rows: 1046\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#dataset information\n",
    "ner_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ttS7ZYp87vTt",
    "outputId": "756ce568-1513-4d99-996a-75d4eb318523"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'id': 6506,\n",
       " 'tokens': ['Later',\n",
       "  'in',\n",
       "  'May',\n",
       "  'of',\n",
       "  '2010',\n",
       "  'within',\n",
       "  'a',\n",
       "  'Pakistani',\n",
       "  'Senate',\n",
       "  'question',\n",
       "  'and',\n",
       "  'answer',\n",
       "  'session',\n",
       "  ',',\n",
       "  'the',\n",
       "  'Gulshan-e-Jinnah',\n",
       "  'Complex',\n",
       "  'was',\n",
       "  'cited',\n",
       "  'under',\n",
       "  'Federal',\n",
       "  'Lodges',\n",
       "  '/',\n",
       "  'Hostels',\n",
       "  'in',\n",
       "  'Islamabad',\n",
       "  'under',\n",
       "  'the',\n",
       "  'control',\n",
       "  'of',\n",
       "  'Pakistan',\n",
       "  'Ministry',\n",
       "  'for',\n",
       "  'Housing',\n",
       "  'and',\n",
       "  'Works',\n",
       "  '.'],\n",
       " 'ner_tags': [6,\n",
       "  6,\n",
       "  6,\n",
       "  6,\n",
       "  6,\n",
       "  6,\n",
       "  6,\n",
       "  6,\n",
       "  6,\n",
       "  6,\n",
       "  6,\n",
       "  6,\n",
       "  6,\n",
       "  6,\n",
       "  6,\n",
       "  6,\n",
       "  6,\n",
       "  6,\n",
       "  6,\n",
       "  6,\n",
       "  6,\n",
       "  6,\n",
       "  6,\n",
       "  6,\n",
       "  6,\n",
       "  6,\n",
       "  6,\n",
       "  6,\n",
       "  6,\n",
       "  6,\n",
       "  6,\n",
       "  6,\n",
       "  6,\n",
       "  6,\n",
       "  6,\n",
       "  6,\n",
       "  6]}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# structure of train data\n",
    "ner_data['train'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "YdMBHXd678Ae",
    "outputId": "3644b871-e152-4bb1-af27-6a4a73414b50"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'id': 1357,\n",
       " 'tokens': ['Stage',\n",
       "  '3',\n",
       "  'exports',\n",
       "  'hundreds',\n",
       "  'of',\n",
       "  'methods',\n",
       "  ',',\n",
       "  'organized',\n",
       "  'into',\n",
       "  '12',\n",
       "  'different',\n",
       "  'major',\n",
       "  'groups',\n",
       "  '.']}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#structure of test data\n",
    "ner_data['test'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "zH9wbwn08B_V",
    "outputId": "b698d713-9c57-4ad3-c49b-53fa524a9163"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'id': 6422,\n",
       " 'tokens': ['Just',\n",
       "  '1',\n",
       "  'year',\n",
       "  'later',\n",
       "  ',',\n",
       "  'after',\n",
       "  'beginning',\n",
       "  'their',\n",
       "  'enterprise',\n",
       "  'on',\n",
       "  '3',\n",
       "  'servers',\n",
       "  'they',\n",
       "  'had',\n",
       "  'filled',\n",
       "  '2',\n",
       "  'server',\n",
       "  'racks',\n",
       "  'with',\n",
       "  'happy',\n",
       "  'clients',\n",
       "  'receiving',\n",
       "  'quality',\n",
       "  'U.S',\n",
       "  'support',\n",
       "  '.'],\n",
       " 'ner_tags': [6,\n",
       "  6,\n",
       "  6,\n",
       "  6,\n",
       "  6,\n",
       "  6,\n",
       "  6,\n",
       "  6,\n",
       "  6,\n",
       "  6,\n",
       "  6,\n",
       "  6,\n",
       "  6,\n",
       "  6,\n",
       "  6,\n",
       "  6,\n",
       "  6,\n",
       "  6,\n",
       "  6,\n",
       "  6,\n",
       "  6,\n",
       "  6,\n",
       "  6,\n",
       "  6,\n",
       "  6,\n",
       "  6]}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#structure of validaion data\n",
    "ner_data['validation'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "D1eWI85q89bI",
    "outputId": "ace24cc7-829a-4bb2-ba9b-9aa2439c31f7"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Sequence(feature=ClassLabel(names=['B-Action', 'B-Entity', 'B-Modifier', 'I-Action', 'I-Entity', 'I-Modifier', 'O'], id=None), length=-1, id=None)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# name of the ner-tags\n",
    "ner_data['train'].features[\"ner_tags\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 139
    },
    "id": "f58wMZ5R93iT",
    "outputId": "ff22f3bb-dfe0-4df6-8324-2b23d9caa436"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'This is the training dataset for Named Entity Recognition (NER).'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#description of the dataset\n",
    "ner_data[\"train\"].info.description = \"This is the training dataset for Named Entity Recognition (NER).\"\n",
    "ner_data[\"validation\"].info.description = \"This is the validation dataset for Named Entity Recognition (NER).\"\n",
    "ner_data[\"test\"].info.description = \"This is the test dataset for Named Entity Recognition (NER).\"\n",
    "\n",
    "\n",
    "ner_data['train'].description"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-F4vj_ns_Z8A"
   },
   "source": [
    "### **Hugging face bert-base-uncased model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "C_CWVSBx_k-P"
   },
   "outputs": [],
   "source": [
    "# intializing tokenizer with help of bert model\n",
    "tokenizer = BertTokenizerFast.from_pretrained(\"bert-large-uncased\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "1k1B3sSADraf",
    "outputId": "57d33cf5-1252-4dd7-ccdd-3a167d000450"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BertTokenizerFast(name_or_path='bert-large-uncased', vocab_size=30522, model_max_length=512, is_fast=True, padding_side='right', truncation_side='right', special_tokens={'unk_token': '[UNK]', 'sep_token': '[SEP]', 'pad_token': '[PAD]', 'cls_token': '[CLS]', 'mask_token': '[MASK]'}, clean_up_tokenization_spaces=False, added_tokens_decoder={\n",
       "\t0: AddedToken(\"[PAD]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t100: AddedToken(\"[UNK]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t101: AddedToken(\"[CLS]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t102: AddedToken(\"[SEP]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t103: AddedToken(\"[MASK]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "}\n",
       ")"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "byS6jZEmCfaM"
   },
   "outputs": [],
   "source": [
    "example_text = ner_data['train'][0]\n",
    "tokenized_input = tokenizer(example_text['tokens'],is_split_into_words=True)\n",
    "tokens = tokenizer.convert_ids_to_tokens(tokenized_input[\"input_ids\"])\n",
    "word_ids = tokenized_input.word_ids()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "JsS8a31SDuir",
    "outputId": "9cb9c494-878c-4481-faed-0db2ac900cc9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': [101, 2101, 1999, 2089, 1997, 2230, 2306, 1037, 9889, 4001, 3160, 1998, 3437, 5219, 1010, 1996, 19739, 4877, 4819, 1011, 1041, 1011, 9743, 15272, 3375, 2001, 6563, 2104, 2976, 26767, 1013, 21071, 2015, 1999, 26905, 2104, 1996, 2491, 1997, 4501, 3757, 2005, 3847, 1998, 2573, 1012, 102], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}\n",
      "\n",
      "\n",
      "['[CLS]', 'later', 'in', 'may', 'of', '2010', 'within', 'a', 'pakistani', 'senate', 'question', 'and', 'answer', 'session', ',', 'the', 'gu', '##ls', '##han', '-', 'e', '-', 'jin', '##nah', 'complex', 'was', 'cited', 'under', 'federal', 'lodges', '/', 'hostel', '##s', 'in', 'islamabad', 'under', 'the', 'control', 'of', 'pakistan', 'ministry', 'for', 'housing', 'and', 'works', '.', '[SEP]']\n",
      "\n",
      "\n",
      "[None, 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 15, 15, 15, 15, 15, 15, 15, 16, 17, 18, 19, 20, 21, 22, 23, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, None]\n"
     ]
    }
   ],
   "source": [
    "print(tokenized_input)\n",
    "print(\"\\n\")\n",
    "print(tokens)\n",
    "print(\"\\n\")\n",
    "print(word_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "yDZHl0xhk9Qc",
    "outputId": "91c1115d-d8ed-459f-ca1c-1154171bce00"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of the tokens is : 47\n",
      "Length of the ner tags is: 37\n"
     ]
    }
   ],
   "source": [
    "print(f'Length of the tokens is : {len(tokens)}')\n",
    "print(f'Length of the ner tags is: {len(ner_data[\"train\"][0][\"ner_tags\"])}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XYC5S4khnmfq"
   },
   "source": [
    "* Here the dimensions of the ner tags and tokens are different , so to make same dimensions of tokens and ner tags we add -100 at the first and last position of the ner tags.\n",
    "\n",
    "* During training , BERT model avoid the -100."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "CHZV9jPE676V"
   },
   "outputs": [],
   "source": [
    "def tokenize_and_align_labels(examples, label_all_tokens=True):\n",
    "    # Vérification du format des données\n",
    "    if isinstance(examples, list):  # Si c'est une liste de dictionnaires\n",
    "        examples = {\n",
    "            \"tokens\": [example[\"tokens\"] for example in examples],\n",
    "            \"ner_tags\": [example[\"ner_tags\"] for example in examples]\n",
    "        }\n",
    "    elif isinstance(examples[\"tokens\"], list) and isinstance(examples[\"tokens\"][0], str):\n",
    "        # Si un seul exemple est fourni\n",
    "        examples = {\n",
    "            \"tokens\": [examples[\"tokens\"]],\n",
    "            \"ner_tags\": [examples[\"ner_tags\"]]\n",
    "        }\n",
    "\n",
    "    # Tokenization\n",
    "    tokenized_inputs = tokenizer(\n",
    "        examples[\"tokens\"],\n",
    "        truncation=True,\n",
    "        is_split_into_words=True\n",
    "    )\n",
    "\n",
    "    labels = []\n",
    "    for i, label in enumerate(examples[\"ner_tags\"]):\n",
    "        word_ids = tokenized_inputs.word_ids(batch_index=i)\n",
    "        previous_word_idx = None\n",
    "        label_ids = []\n",
    "        for word_idx in word_ids:\n",
    "            if word_idx is None:\n",
    "                label_ids.append(-100)\n",
    "            elif word_idx != previous_word_idx:\n",
    "                label_ids.append(label[word_idx])\n",
    "            else:\n",
    "                label_ids.append(label[word_idx] if label_all_tokens else -100)\n",
    "            previous_word_idx = word_idx\n",
    "        labels.append(label_ids)\n",
    "\n",
    "    tokenized_inputs[\"labels\"] = labels\n",
    "    return tokenized_inputs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "6rynLUil9nnk",
    "outputId": "862da837-f2db-4be3-ab77-bb669f46b345"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'id': [3114],\n",
       " 'tokens': [['The',\n",
       "   \"regime's\",\n",
       "   'CSTIA',\n",
       "   'relies',\n",
       "   'on',\n",
       "   'Russia',\n",
       "   'as',\n",
       "   'one',\n",
       "   'of',\n",
       "   'several',\n",
       "   'sources',\n",
       "   'for',\n",
       "   'technical',\n",
       "   'data',\n",
       "   '.']],\n",
       " 'ner_tags': [[6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6]]}"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ner_data['train'][4:5]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "MydrcSiI84lu",
    "outputId": "06e06ec0-6bf4-4442-91f2-15724fecaef3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': [[101, 1996, 6939, 1005, 1055, 20116, 10711, 16803, 2006, 3607, 2004, 2028, 1997, 2195, 4216, 2005, 4087, 2951, 1012, 102]], 'token_type_ids': [[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]], 'attention_mask': [[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]], 'labels': [[-100, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, -100]]}\n"
     ]
    }
   ],
   "source": [
    "q = tokenize_and_align_labels(ner_data['train'][4:5])\n",
    "print(q)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GI_JN_8zHGGq"
   },
   "source": [
    "So before applying the tokenize_and_align_labels() the tokenized_input has 3 keys\n",
    "\n",
    "* input_ids\n",
    "* token_type_ids\n",
    "* attention_mask\n",
    "\n",
    "But after applying tokenize_and_align_labels() we have an extra key - 'labels'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "K1zIT2clDYIF",
    "outputId": "459115f2-2053-4d34-fc37-175c4db28b13"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CLS]___________________________________ -100\n",
      "the_____________________________________ 6\n",
      "regime__________________________________ 6\n",
      "'_______________________________________ 6\n",
      "s_______________________________________ 6\n",
      "cs______________________________________ 6\n",
      "##tia___________________________________ 6\n",
      "relies__________________________________ 6\n",
      "on______________________________________ 6\n",
      "russia__________________________________ 6\n",
      "as______________________________________ 6\n",
      "one_____________________________________ 6\n",
      "of______________________________________ 6\n",
      "several_________________________________ 6\n",
      "sources_________________________________ 6\n",
      "for_____________________________________ 6\n",
      "technical_______________________________ 6\n",
      "data____________________________________ 6\n",
      "._______________________________________ 6\n",
      "[SEP]___________________________________ -100\n"
     ]
    }
   ],
   "source": [
    "for token, label in zip(tokenizer.convert_ids_to_tokens(q[\"input_ids\"][0]),q[\"labels\"][0]):\n",
    "    print(f\"{token:_<40} {label}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "id": "Rep2O2V2Hvqu"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1ab81ab7fc884e6cac8c154190a3a236",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/4876 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1524b53cf4774e6f915aa9d68a64195c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1044 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['id', 'tokens', 'ner_tags', 'input_ids', 'token_type_ids', 'attention_mask', 'labels'],\n",
      "        num_rows: 4876\n",
      "    })\n",
      "    validation: Dataset({\n",
      "        features: ['id', 'tokens', 'ner_tags', 'input_ids', 'token_type_ids', 'attention_mask', 'labels'],\n",
      "        num_rows: 1044\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['id', 'tokens'],\n",
      "        num_rows: 1046\n",
      "    })\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "# Appliquer .map() uniquement sur \"train\" et \"validation\"\n",
    "tokenized_datasets = DatasetDict({\n",
    "    key: dataset.map(tokenize_and_align_labels, batched=True) if key != \"test\" else dataset\n",
    "    for key, dataset in ner_data.items()\n",
    "})\n",
    "\n",
    "# Vérification\n",
    "print(tokenized_datasets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "rMgL_9oHH4dO",
    "outputId": "a3541c34-a8e8-4077-eb61-9d11187be7ab"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'id': 6506,\n",
       " 'tokens': ['Later',\n",
       "  'in',\n",
       "  'May',\n",
       "  'of',\n",
       "  '2010',\n",
       "  'within',\n",
       "  'a',\n",
       "  'Pakistani',\n",
       "  'Senate',\n",
       "  'question',\n",
       "  'and',\n",
       "  'answer',\n",
       "  'session',\n",
       "  ',',\n",
       "  'the',\n",
       "  'Gulshan-e-Jinnah',\n",
       "  'Complex',\n",
       "  'was',\n",
       "  'cited',\n",
       "  'under',\n",
       "  'Federal',\n",
       "  'Lodges',\n",
       "  '/',\n",
       "  'Hostels',\n",
       "  'in',\n",
       "  'Islamabad',\n",
       "  'under',\n",
       "  'the',\n",
       "  'control',\n",
       "  'of',\n",
       "  'Pakistan',\n",
       "  'Ministry',\n",
       "  'for',\n",
       "  'Housing',\n",
       "  'and',\n",
       "  'Works',\n",
       "  '.'],\n",
       " 'ner_tags': [6,\n",
       "  6,\n",
       "  6,\n",
       "  6,\n",
       "  6,\n",
       "  6,\n",
       "  6,\n",
       "  6,\n",
       "  6,\n",
       "  6,\n",
       "  6,\n",
       "  6,\n",
       "  6,\n",
       "  6,\n",
       "  6,\n",
       "  6,\n",
       "  6,\n",
       "  6,\n",
       "  6,\n",
       "  6,\n",
       "  6,\n",
       "  6,\n",
       "  6,\n",
       "  6,\n",
       "  6,\n",
       "  6,\n",
       "  6,\n",
       "  6,\n",
       "  6,\n",
       "  6,\n",
       "  6,\n",
       "  6,\n",
       "  6,\n",
       "  6,\n",
       "  6,\n",
       "  6,\n",
       "  6],\n",
       " 'input_ids': [101,\n",
       "  2101,\n",
       "  1999,\n",
       "  2089,\n",
       "  1997,\n",
       "  2230,\n",
       "  2306,\n",
       "  1037,\n",
       "  9889,\n",
       "  4001,\n",
       "  3160,\n",
       "  1998,\n",
       "  3437,\n",
       "  5219,\n",
       "  1010,\n",
       "  1996,\n",
       "  19739,\n",
       "  4877,\n",
       "  4819,\n",
       "  1011,\n",
       "  1041,\n",
       "  1011,\n",
       "  9743,\n",
       "  15272,\n",
       "  3375,\n",
       "  2001,\n",
       "  6563,\n",
       "  2104,\n",
       "  2976,\n",
       "  26767,\n",
       "  1013,\n",
       "  21071,\n",
       "  2015,\n",
       "  1999,\n",
       "  26905,\n",
       "  2104,\n",
       "  1996,\n",
       "  2491,\n",
       "  1997,\n",
       "  4501,\n",
       "  3757,\n",
       "  2005,\n",
       "  3847,\n",
       "  1998,\n",
       "  2573,\n",
       "  1012,\n",
       "  102],\n",
       " 'token_type_ids': [0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0],\n",
       " 'attention_mask': [1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1],\n",
       " 'labels': [-100,\n",
       "  6,\n",
       "  6,\n",
       "  6,\n",
       "  6,\n",
       "  6,\n",
       "  6,\n",
       "  6,\n",
       "  6,\n",
       "  6,\n",
       "  6,\n",
       "  6,\n",
       "  6,\n",
       "  6,\n",
       "  6,\n",
       "  6,\n",
       "  6,\n",
       "  6,\n",
       "  6,\n",
       "  6,\n",
       "  6,\n",
       "  6,\n",
       "  6,\n",
       "  6,\n",
       "  6,\n",
       "  6,\n",
       "  6,\n",
       "  6,\n",
       "  6,\n",
       "  6,\n",
       "  6,\n",
       "  6,\n",
       "  6,\n",
       "  6,\n",
       "  6,\n",
       "  6,\n",
       "  6,\n",
       "  6,\n",
       "  6,\n",
       "  6,\n",
       "  6,\n",
       "  6,\n",
       "  6,\n",
       "  6,\n",
       "  6,\n",
       "  6,\n",
       "  -100]}"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_datasets['train'][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OA6epGsI-UUu"
   },
   "source": [
    "### **Defining the model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "VTJvJnJ_Q3hu",
    "outputId": "4615330d-968b-4cc7-a9c6-f326586caf5d"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-large-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "BertForTokenClassification(\n",
       "  (bert): BertModel(\n",
       "    (embeddings): BertEmbeddings(\n",
       "      (word_embeddings): Embedding(30522, 1024, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 1024)\n",
       "      (token_type_embeddings): Embedding(2, 1024)\n",
       "      (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): BertEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0-23): 24 x BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSdpaSelfAttention(\n",
       "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (dropout): Dropout(p=0.1, inplace=False)\n",
       "  (classifier): Linear(in_features=1024, out_features=7, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Defining model\n",
    "ner_model = AutoModelForTokenClassification.from_pretrained(\"bert-large-uncased\", num_labels=7)\n",
    "ner_model.to(DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 964
    },
    "id": "vTbkxxPKBVN5",
    "outputId": "d4666eb6-3ff6-4193-9173-8b516a886e7c"
   },
   "outputs": [],
   "source": [
    "#! pip install -U accelerate\n",
    "#! pip install -U transformers\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 513
    },
    "id": "lPW6ipUWB-mu",
    "outputId": "8af35904-4fd6-4aea-d55b-c99837fbd8d3"
   },
   "outputs": [],
   "source": [
    "#!pip install accelerate\n",
    "#!pip install 'accelerate>=0.20.1,<0.21'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "jJ5hewXl_-5y",
    "outputId": "3e86d4f9-fbdf-4877-edc4-4bb7b903fc6a"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('4.47.1', '1.2.1')"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "transformers.__version__, accelerate.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "id": "Sg67bEBl_0uc"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TrainingArguments created successfully!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "e:\\programmation\\python\\.ma511\\Lib\\site-packages\\transformers\\training_args.py:1575: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "#Define training args\n",
    "args = TrainingArguments(\n",
    "    \"test-ner\",\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    learning_rate=LEARNING_RATE,\n",
    "    per_device_train_batch_size=TRAIN_BATCH_SIZE,\n",
    "    per_device_eval_batch_size=VALID_BATCH_SIZE,\n",
    "    num_train_epochs=EPOCHS,\n",
    "    weight_decay=0.01,\n",
    "    save_strategy=\"epoch\",\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"f1\",\n",
    "    greater_is_better=True,\n",
    ")\n",
    "\n",
    "\n",
    "print(\"TrainingArguments created successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "id": "0bLctsQoDBDA"
   },
   "outputs": [],
   "source": [
    "data_collator = DataCollatorForTokenClassification(tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 104,
     "referenced_widgets": [
      "b85487cb3cbc4c1da822a0c036b8fdea",
      "2cb94704cf66435b81ff24d2a8fb7b06",
      "f19da5a9934a4603bb25ecc8baef6694",
      "8d03c2eed255421c8f791d05fb99a688",
      "9cc8553f85684cc784904ed5399d3322",
      "0c843dbafcfc4224b4a6adbc7a1dac33",
      "85fd2ee8a6ed43118e85227db73a8c3b",
      "ee8c3e33b24e48dcbea2a9cb8051842b",
      "92e463fd9c784f79bf09f07831e881fa",
      "c7aff299189e42ee8b0b43fcb0a7f85b",
      "fa4f4f84a5f74d90b56176b407717540"
     ]
    },
    "id": "jXRm1FC0DHa9",
    "outputId": "1533f64a-a8fa-403d-bddb-51f698d3f2aa"
   },
   "outputs": [],
   "source": [
    "import evaluate\n",
    "\n",
    "# Charger une métrique, par exemple, \"seqeval\"\n",
    "metric = evaluate.load(\"seqeval\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "UZ-RwQozG6Hj",
    "outputId": "1a8ac4f2-b855-49c2-f6a0-33da20507301"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['B-Action',\n",
       " 'B-Entity',\n",
       " 'B-Modifier',\n",
       " 'I-Action',\n",
       " 'I-Entity',\n",
       " 'I-Modifier',\n",
       " 'O']"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label_list = ner_data[\"train\"].features[\"ner_tags\"].feature.names\n",
    "label_list\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VegBjEVSGpW6"
   },
   "source": [
    "Compute Metrics\n",
    "\n",
    "This compute_metrics() function first takes the argmax of the logits to convert them to predictions (as usual, the logits and the probabilities are in the same order, so we don’t need to apply the softmax). Then we have to convert both labels and predictions from integers to strings. We remove all the values where the label is -100, then pass the results to the metric.compute() method:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "id": "xnHZXoRUGolA"
   },
   "outputs": [],
   "source": [
    "def compute_metrics(eval_preds):\n",
    "    pred_logits, labels = eval_preds\n",
    "    print(eval_preds)\n",
    "\n",
    "    pred_logits = np.argmax(pred_logits, axis=2)\n",
    "    # the logits and the probabilities are in the same order,\n",
    "    # so we don’t need to apply the softmax\n",
    "\n",
    "    # We remove all the values where the label is -100\n",
    "    predictions = [\n",
    "        [label_list[eval_preds] for (eval_preds, l) in zip(prediction, label) if l != -100]\n",
    "        for prediction, label in zip(pred_logits, labels)\n",
    "    ]\n",
    "\n",
    "    true_labels = [\n",
    "      [label_list[l] for (eval_preds, l) in zip(prediction, label) if l != -100]\n",
    "       for prediction, label in zip(pred_logits, labels)\n",
    "   ]\n",
    "    results = metric.compute(predictions=predictions, references=true_labels)\n",
    "\n",
    "    return {\n",
    "          \"precision\": results[\"overall_precision\"],\n",
    "          \"recall\": results[\"overall_recall\"],\n",
    "          \"f1\": results[\"overall_f1\"],\n",
    "          \"accuracy\": results[\"overall_accuracy\"],\n",
    "  }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AthC4bjYHoBk"
   },
   "source": [
    "### **Train the model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "id": "d8UI3OCnHrb_"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Rsky\\AppData\\Local\\Temp\\ipykernel_28212\\2818633497.py:1: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n"
     ]
    }
   ],
   "source": [
    "trainer = Trainer(\n",
    "   ner_model,\n",
    "   args,\n",
    "   train_dataset=tokenized_datasets[\"train\"],\n",
    "   eval_dataset=tokenized_datasets[\"validation\"],\n",
    "   data_collator=data_collator,\n",
    "   tokenizer=tokenizer,\n",
    "   compute_metrics=compute_metrics\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 196
    },
    "id": "P7oxkSxAH5AH",
    "outputId": "ce04cfb9-64d5-4b10-9aa9-27ede7f11f86"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "26fcc12b8bbe4b529ba68cb5a642113e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/459 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_89FmP8V0xaz"
   },
   "source": [
    "### **Save Model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "id": "nVxC4F_I00G8"
   },
   "outputs": [],
   "source": [
    "## Save model\n",
    "ner_model.save_pretrained(\"ner_model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Y3GcxDEK09JF",
    "outputId": "eae1ab28-ecbd-444f-adc9-c5e3cf80508a"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('tokenizer\\\\tokenizer_config.json',\n",
       " 'tokenizer\\\\special_tokens_map.json',\n",
       " 'tokenizer\\\\vocab.txt',\n",
       " 'tokenizer\\\\added_tokens.json',\n",
       " 'tokenizer\\\\tokenizer.json')"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Save tokenizer\n",
    "tokenizer.save_pretrained(\"tokenizer\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "id": "BzW3WumK1EeD"
   },
   "outputs": [],
   "source": [
    "id2label = {\n",
    "    str(i): label for i,label in enumerate(label_list)\n",
    "}\n",
    "label2id = {\n",
    "    label: str(i) for i,label in enumerate(label_list)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "TjnsUP3B1Nxb",
    "outputId": "cdeaf9a2-41b4-4477-e475-695c68cc73cd"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'0': 'B-Action',\n",
       " '1': 'B-Entity',\n",
       " '2': 'B-Modifier',\n",
       " '3': 'I-Action',\n",
       " '4': 'I-Entity',\n",
       " '5': 'I-Modifier',\n",
       " '6': 'O'}"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "id2label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "uYYhwQeE1QL5",
    "outputId": "7681d5e6-4f16-4fed-c8c4-2a0eb23887f7"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'B-Action': '0',\n",
       " 'B-Entity': '1',\n",
       " 'B-Modifier': '2',\n",
       " 'I-Action': '3',\n",
       " 'I-Entity': '4',\n",
       " 'I-Modifier': '5',\n",
       " 'O': '6'}"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label2id"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5BhH95zc1lpY"
   },
   "source": [
    "### **Load model & predicton**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "id": "KQCFo4981sJM"
   },
   "outputs": [],
   "source": [
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "id": "MHjj2Xqw11pu"
   },
   "outputs": [],
   "source": [
    "config = json.load(open(\"ner_model/config.json\"))\n",
    "config[\"id2label\"] = id2label\n",
    "config[\"label2id\"] = label2id\n",
    "json.dump(config, open(\"ner_model/config.json\",\"w\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "id": "lEbj3cqexqxm"
   },
   "outputs": [],
   "source": [
    "model_fine_tuned = AutoModelForTokenClassification.from_pretrained(\"ner_model\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "id": "5hJYTXC7x_j2"
   },
   "outputs": [],
   "source": [
    "from transformers import pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "kMotAseQyE5D",
    "outputId": "eecd7727-7ad5-44c0-f280-4c59e9d8f7ab"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n",
      "You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prédictions enregistrées dans ./data/NER-TESTING-PREDICTED.jsonlines\n",
      "Prédictions enregistrées dans ./data/NER-VALIDATION-PREDICTED.jsonlines\n",
      "Résultats d'évaluation :\n",
      "{\n",
      "  \"Action\": {\n",
      "    \"precision\": 0.6049046321525886,\n",
      "    \"recall\": 0.5336538461538461,\n",
      "    \"f1\": 0.5670498084291188,\n",
      "    \"number\": \"416\"\n",
      "  },\n",
      "  \"Entity\": {\n",
      "    \"precision\": 0.10482019892884469,\n",
      "    \"recall\": 0.14842903575297942,\n",
      "    \"f1\": 0.12286995515695068,\n",
      "    \"number\": \"923\"\n",
      "  },\n",
      "  \"Modifier\": {\n",
      "    \"precision\": 0.6015037593984962,\n",
      "    \"recall\": 0.5714285714285714,\n",
      "    \"f1\": 0.586080586080586,\n",
      "    \"number\": \"280\"\n",
      "  },\n",
      "  \"overall_precision\": 0.2675257731958763,\n",
      "  \"overall_recall\": 0.32056825200741196,\n",
      "  \"overall_f1\": 0.29165495925821855,\n",
      "  \"overall_accuracy\": 0.8628137540325753\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "# Labels définis dans votre modèle\n",
    "ner_labels = [\"B-Action\", \"B-Entity\", \"B-Modifier\", \"I-Action\", \"I-Entity\", \"I-Modifier\", \"O\"]\n",
    "\n",
    "# Initialiser le pipeline NER\n",
    "nlp = pipeline(\"ner\", model=model_fine_tuned.to(DEVICE), tokenizer=tokenizer, device=0 if DEVICE == 'cuda' else -1)\n",
    "\n",
    "# Fonction pour convertir les indices en labels\n",
    "def convert_indices_to_labels(indices, label_list):\n",
    "    \"\"\"\n",
    "    Convertit une liste d'indices en étiquettes à l'aide de la liste de labels.\n",
    "    \"\"\"\n",
    "    return [label_list[int(idx)] for idx in indices]  # Conversion explicite en int\n",
    "\n",
    "# Fonction pour générer les prédictions et écrire dans un fichier JSONlines\n",
    "def predict_and_save(ner_dataset, output_file):\n",
    "    results = []\n",
    "    for example in ner_dataset:\n",
    "        # Effectuer les prédictions sur les tokens\n",
    "        tokens = example[\"tokens\"]\n",
    "        ner_results = nlp(\" \".join(tokens))\n",
    "        \n",
    "        # Initialiser les ner_tags prédits\n",
    "        ner_tags_predicted = [\"O\"] * len(tokens)\n",
    "        \n",
    "        for ner_result in ner_results:\n",
    "            label = ner_result[\"entity\"]\n",
    "            \n",
    "            # Trouver le mot correspondant au résultat NER\n",
    "            word = ner_result[\"word\"]\n",
    "            try:\n",
    "                token_idx = tokens.index(word)\n",
    "                ner_tags_predicted[token_idx] = label\n",
    "            except ValueError:\n",
    "                # Si le mot ne correspond pas, continuez\n",
    "                continue\n",
    "\n",
    "        # Ajouter le résultat au format JSONlines\n",
    "        results.append({\n",
    "            \"unique_id\": int(example[\"id\"]),  # Conversion explicite en int\n",
    "            \"tokens\": tokens,\n",
    "            \"ner_tags\": convert_indices_to_labels(example.get(\"ner_tags\", []), ner_labels),  # Conversion des indices\n",
    "            \"ner_tags_predicted\": ner_tags_predicted\n",
    "        })\n",
    "    \n",
    "    # Écrire les résultats dans le fichier JSONlines\n",
    "    with open(output_file, \"w\", encoding=\"utf-8\") as f:\n",
    "        for item in results:\n",
    "            f.write(json.dumps(item) + \"\\n\")\n",
    "    print(f\"Prédictions enregistrées dans {output_file}\")\n",
    "\n",
    "# Effectuer les prédictions pour le jeu de test\n",
    "predict_and_save(ner_data[\"test\"], \"./data/NER-TESTING-PREDICTED.jsonlines\")\n",
    "\n",
    "# Effectuer les prédictions pour le jeu de validation\n",
    "predict_and_save(ner_data[\"validation\"], \"./data/NER-VALIDATION-PREDICTED.jsonlines\")\n",
    "\n",
    "# Fonction pour évaluer les prédictions\n",
    "def evaluate_predictions(validation_file, label_list):\n",
    "    \"\"\"\n",
    "    Évalue les prédictions en comparant les ner_tags_predicted aux ner_tags.\n",
    "    \"\"\"\n",
    "    # Charger les prédictions\n",
    "    with open(validation_file, \"r\", encoding=\"utf-8\") as f:\n",
    "        examples = [json.loads(line) for line in f]\n",
    "\n",
    "    metric = evaluate.load(\"seqeval\")\n",
    "    all_predictions = []\n",
    "    all_references = []\n",
    "\n",
    "    for example in examples:\n",
    "        if \"ner_tags\" in example and \"ner_tags_predicted\" in example:\n",
    "            references = example[\"ner_tags\"]\n",
    "            predictions = example[\"ner_tags_predicted\"]\n",
    "\n",
    "            all_predictions.append(predictions)\n",
    "            all_references.append(references)\n",
    "\n",
    "    # Calcul des métriques\n",
    "    results = metric.compute(predictions=all_predictions, references=all_references)\n",
    "    print(\"Résultats d'évaluation :\")\n",
    "    print(json.dumps(results, indent=2, default=str))  # Ajout de default=str\n",
    "    return results\n",
    "\n",
    "# Évaluer sur le jeu de validation\n",
    "validation_results = evaluate_predictions(\"./data/NER-VALIDATION-PREDICTED.jsonlines\", ner_labels)\n"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "authorship_tag": "ABX9TyOcam7pE37gFYFHflIUvXDi",
   "gpuType": "T4",
   "include_colab_link": true,
   "provenance": []
  },
  "kernelspec": {
   "display_name": "clean_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "0c843dbafcfc4224b4a6adbc7a1dac33": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "2cb94704cf66435b81ff24d2a8fb7b06": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_0c843dbafcfc4224b4a6adbc7a1dac33",
      "placeholder": "​",
      "style": "IPY_MODEL_85fd2ee8a6ed43118e85227db73a8c3b",
      "value": "Downloading builder script: "
     }
    },
    "85fd2ee8a6ed43118e85227db73a8c3b": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "8d03c2eed255421c8f791d05fb99a688": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_c7aff299189e42ee8b0b43fcb0a7f85b",
      "placeholder": "​",
      "style": "IPY_MODEL_fa4f4f84a5f74d90b56176b407717540",
      "value": " 6.33k/? [00:00&lt;00:00, 234kB/s]"
     }
    },
    "92e463fd9c784f79bf09f07831e881fa": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "9cc8553f85684cc784904ed5399d3322": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "b85487cb3cbc4c1da822a0c036b8fdea": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_2cb94704cf66435b81ff24d2a8fb7b06",
       "IPY_MODEL_f19da5a9934a4603bb25ecc8baef6694",
       "IPY_MODEL_8d03c2eed255421c8f791d05fb99a688"
      ],
      "layout": "IPY_MODEL_9cc8553f85684cc784904ed5399d3322"
     }
    },
    "c7aff299189e42ee8b0b43fcb0a7f85b": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "ee8c3e33b24e48dcbea2a9cb8051842b": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "f19da5a9934a4603bb25ecc8baef6694": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_ee8c3e33b24e48dcbea2a9cb8051842b",
      "max": 2472,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_92e463fd9c784f79bf09f07831e881fa",
      "value": 2472
     }
    },
    "fa4f4f84a5f74d90b56176b407717540": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
